从本专栏开始，作者正式开始研究Python深度学习、神经网络及人工智能相关知识。前一篇文章详细讲解了如何评价神经网络，绘制训练过程中的loss曲线，并结合图像分类案例讲解精确率、召回率和F值的计算过程。本篇文章将分享循环神经网络LSTM RNN如何实现回归预测，通过sin曲线拟合实现如下图所示效果。本文代码量比较长，但大家还是可以学习下的。基础性文章，希望对您有所帮助！
本专栏主要结合作者之前的博客、AI经验和相关视频（强推"莫烦大神"视频）及论文介绍，后面随着深入会讲解更多的Python人工智能案例及应用。基础性文章，希望对您有所帮助，如果文章中存在错误或不足之处，还请海涵~作者作为人工智能的菜鸟，希望大家能与我在这一笔一划的博客中成长起来。写了这么多年博客，尝试第一个付费专栏，但更多博客尤其基础性文章，还是会继续免费分享，但该专栏也会用心撰写，望对得起读者，共勉！
代码下载地址：

CSDN下载地址：

PS：百度网盘链接总被下线，需要的私聊我，或从CSDN、Github下载。
同时推荐前面作者另外三个Python系列文章。从2014年开始，作者主要写了三个Python系列文章，分别是基础知识、网络爬虫和数据分析。2018年陆续增加了Python图像识别和Python人工智能专栏。
前文：


























在编写代码之前，我们再回归下RNN和LSTM的原理知识，因为部分博友可能没看过之前 
。

循环神经网络英文是Recurrent Neural Networks，简称RNN。假设有一组数据data0、data1、data2、data3，使用同一个神经网络预测它们，得到对应的结果。如果数据之间是有关系的，比如做菜下料的前后步骤，英文单词的顺序，如何让数据之间的关联也被神经网络学习呢？这就要用到——RNN。
假设存在ABCD数字，需要预测下一个数字E，会根据前面ABCD顺序进行预测，这就称为记忆。预测之前，需要回顾以前的记忆有哪些，再加上这一步新的记忆点，最终输出output，循环神经网络（RNN）就利用了这样的原理。
首先，让我们想想人类是怎么分析事物之间的关联或顺序的。人类通常记住之前发生的事情，从而帮助我们后续的行为判断，那么是否能让计算机也记住之前发生的事情呢？
在分析data0时，我们把分析结果存入记忆Memory中，然后当分析data1时，神经网络（NN）会产生新的记忆，但此时新的记忆和老的记忆没有关联，如上图所示。在RNN中，我们会简单的把老记忆调用过来分析新记忆，如果继续分析更多的数据时，NN就会把之前的记忆全部累积起来。
RNN结构如下图所示，按照时间点t-1、t、t+1，每个时刻有不同的x，每次计算会考虑上一步的state和这一步的x(t)，再输出y值。在该数学形式中，每次RNN运行完之后都会产生s(t)，当RNN要分析x(t+1)时，此刻的y(t+1)是由s(t)和s(t+1)共同创造的，s(t)可看作上一步的记忆。多个神经网络NN的累积就转换成了循环神经网络，其简化图如下图的左边所示。
总之，只要你的数据是有顺序的，就可以使用RNN，比如人类说话的顺序，电话号码的顺序，图像像素排列的顺序，ABC字母的顺序等。在前面讲解CNN原理时，它可以看做是一个滤波器滑动扫描整幅图像，通过卷积加深神经网络对图像的理解。
而RNN也有同样的扫描效果，只不过是增加了时间顺序和记忆功能。RNN通过隐藏层周期性的连接，从而捕获序列化数据中的动态信息，提升预测结果。

RNN常用于自然语言处理、机器翻译、语音识别、图像识别等领域，下面简单分享RNN相关应用所对应的结构。
接下来我们看一个更强大的结构，称为LSTM。
RNN是在有序的数据上进行学习的，RNN会像人一样对先前的数据发生记忆，但有时候也会像老爷爷一样忘记先前所说。为了解决RNN的这个弊端，提出了LTSM技术，它的英文全称是Long short-term memory，长短期记忆，也是当下最流行的RNN之一。
假设现在有一句话，如下图所示，RNN判断这句话是红烧排骨，这时需要学习，而“红烧排骨“在句子开头。
"红烧排骨"这个词需要经过长途跋涉才能抵达，要经过一系列得到误差，然后经过反向传递，它在每一步都会乘以一个权重w参数。如果乘以的权重是小于1的数，比如0.9，0.9会不断地乘以误差，最终这个值传递到初始值时，误差就消失了，这称为梯度消失或梯度离散。
反之，如果误差是一个很大的数，比如1.1，则这个RNN得到的值会很大，这称为梯度爆炸。
这也是RNN没有恢复记忆的原因，为了解决RNN梯度下降时遇到的梯度消失或梯度爆炸问题，引入了LSTM。
LSTM是在普通的RNN上面做了一些改进，LSTM RNN多了三个控制器，即输入、输出、忘记控制器。左边多了个条主线，例如电影的主线剧情，而原本的RNN体系变成了分线剧情，并且三个控制器都在分线上。
LSTM工作原理为：如果分线剧情对于最终结果十分重要，输入控制器会将这个分线剧情按重要程度写入主线剧情，再进行分析；如果分线剧情改变了我们之前的想法，那么忘记控制器会将某些主线剧情忘记，然后按比例替换新剧情，所以主线剧情的更新就取决于输入和忘记控制；最后的输出会基于主线剧情和分线剧情。
通过这三个gate能够很好地控制我们的RNN，基于这些控制机制，LSTM是延缓记忆的良药，从而带来更好的结果。
前面我们讲解了RNN、CNN的分类问题，这篇文章将分享一个回归问题。在LSTM RNN回归案例中，我们想要用蓝色的虚线预测红色的实线，由于sin曲线是波浪循环，所以RNN会用一段序列来预测另一段序列。
代码基本结构包括：
最后再补充下BPTT，就开始我们的代码编写。

假设我们训练含有1000000个数据的序列，如果全部训练的话，整个的序列都feed进RNN中，容易造成梯度消失或爆炸的问题。所以解决的方法就是截断反向传播 (Truncated Backpropagation，BPTT) ，我们将序列截断来进行训练(num_steps)。
一般截断的反向传播是：在当前时间t，往前反向传播num_steps步即可。如下图，长度为6的序列，截断步数是3，Initial State和Final State在RNN Cell中传递。

但是Tensorflow中的实现并不是这样，它是将长度为6的序列分为了两部分，每一部分长度为3，前一部分计算得到的final state用于下一部分计算的initial state。如下图所示，每个batch进行单独的截断反向传播。此时的batch会保存Final State，并作为下一个batch的初始化State。
参考：
此时的输出结果如下图所示，注意它只是模拟的预期曲线，还不是我们神经网络学习的结构。
初始化init()函数的参数包括：
该部分代码如下，注意xs和ys的形状。同时，我们需要使用Tensorboard可视化RNN的结构，所以调用tf.name_scope()设置各神经层和变量的命名空间名称，详见 
。
这三个函数也是增加在LSTMRNN的Class中，核心代码及详细注释如下所示：
注意，上面调用了reshape()进行形状更新，为什么要将三维变量改成二维呢？因为只有变成二维变量之后，才能计算W*X+B。

这里需要注意：我们使用了seq2seq函数。它求出的loss是整个batch每一步的loss，然后把每一步loss进行sum求和，变成了整个TensorFlow的loss，再除以batch size平均，最终得到这个batch的总cost，它是一个scalar数字。
后面的文章我们会详细写一篇机器翻译相关的内容，并使用seq2seq模型。
写到这里，整个Class就定义完成。
该阶段的完整代码如下，我们先尝试运行下代码：
此时会在Python文件目录下新建一个“logs”文件夹和events的文件，如下图所示。
接下来尝试打开它。首先调出Anaconda Prompt，并激活TensorFlow，接着去到events文件的目录，调用命令“tensorboard --logdir=logs运行即可，如下图所示。注意，这里只需要指引到文件夹，它就会自动索引到你的文件。
此时访问网址“http://localhost:6006/”，选择“Graphs”，运行之后如下图所示，我们的神经网络就出现了。
神经网络结构如下图所示，包括输入层、LSTM层、输出层、cost误差计算、train训练等。
详细结构如下图所示：
通常我们会将train部分放置一边，选中“train”然后鼠标右键点击“Remove from main graph”。核心结构如下，in_hidden是接受输入的第一层，之后是LSTM_cell，最后是输出层out_hidden。
如果您报错 AttributeError: module ‘tensorflow._api.v1.nn’ has no attribute ‘seq2seq’，这是TensorFlow 版本升级，方法调用更改。解决方式：
如果您报错 TypeError: msr_error() got an unexpected keyword argument ‘labels’，msr_error() 函数得到一个意外的关键参数 ‘lables’。其解决方式：定义msr_error() 函数时，使用 labels，logits 指定，将
改为：
如果您报错 ValueError: Variable in_hidden/weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? ，则重新启动kernel即可运行。
最后，我们在主函数中编写RNN训练学习和预测的代码。
首先我们来测试cost学习的结果。代码如下，if判断中cell_init_state为前面已初始化的state，之后更新state（model.cell_init_state: state  ），其实就是将Final State换成下一个batch的Initial State，从而符合我们定义的结构。
每隔20步输出结果，如下所示，误差从最初的33到最后的0.335，神经网络在不断学习，误差在不断减小。
接下来增加matplotlib可视化的sin曲线动态拟合过程，最终完整代码如下所示：
写道这里，这篇文章终于写完了。文章非常长，但希望对您有所帮助。LSTM RNN通过一组数据预测另一组数据。预测效果如下图所示，红色的实线表示需要预测的线，蓝色的虚线表示RNN学习的线，它们在不断地逼近，蓝线学到了红线的规律，最终将蓝线基本拟合到红线上。
本文介绍完了，更多TensorFlow深度学习文章会继续分享，接下来我们会分享监督学习、GAN、机器翻译、文本识别、图像识别、语音识别等内容。如果读者有什么想学习的，也可以私聊我，我去学习并应用到你的领域。
最后，希望这篇基础性文章对您有所帮助，如果文章中存在错误或不足之处，还请海涵~作为人工智能的菜鸟，我希望自己能不断进步并深入，后续将它应用于图像识别、网络安全、对抗样本等领域，指导大家撰写简单的学术论文，一起加油！
希望大家帮我CSDN博客之星投投票，每天可以投5票喔，谢谢大家！八年，在CSDN分享了410篇文章，15个专栏，400多万人次浏览，包括Python人工智能、数据挖掘、网络爬虫、图象处理、网络安全、JAVA网站、Android开发、LAMP/WAMP、C#网络编程、C++游戏、算法和数据结构、面试总结、人生感悟等。当然还有我和你的故事，感恩一路有你，感谢一路同行，希望通过编程分享帮助到更多人，尤其初学者，也希望学成之后回贵州教更多学生。因为喜欢，所以分享，且看且珍惜，加油！等我学成归来～（微信、QQ、微博扫二维码即可投票）
PS：这是作者的第一个付费专栏，会非常用心的去撰写，写了八年的免费文章，这也算知识付费的一个简单尝试吧！毕竟读博也不易，写文章也花费时间和精力，但作者更多的文章会免费分享。如果您购买了该专栏，有Python数据分析、图像处理、人工智能、网络安全的问题，我们都可以深入探讨，尤其是做研究的同学，共同进步~
(By:Eastmount 2020-01-10 中午13点夜于珞珈山 
 )















[1] 杨秀璋, 颜娜. Python网络数据爬取及分析从入门到精通（分析篇）[M]. 北京：北京航天航空大学出版社, 2018.

[2] 

[3] 

[4] 

[5] 

[6]《机器学习》周志华

[7] 

[8] 

[9] 

[10] 

[11] 

[12] 

[13] 
